{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUJkdeLSV5jjIEFL3Tu4+x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9177330004944c109d805d660d3494dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_b2177a658b5f4b6c8b476ed59b978416",
            "style": "IPY_MODEL_70f100b00dfe428bb5b1112b97e5582f",
            "tooltip": ""
          }
        },
        "b2177a658b5f4b6c8b476ed59b978416": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f100b00dfe428bb5b1112b97e5582f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "98d479ffe4a243b28dbe3f26ec8b2358": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_f222a55082984397ad78ed8f3e5a9802",
            "style": "IPY_MODEL_32705baac7204f2fa8266dce2e76003d",
            "tooltip": ""
          }
        },
        "f222a55082984397ad78ed8f3e5a9802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32705baac7204f2fa8266dce2e76003d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7b815f5396f94e90b3c40747fed36ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✘ Downloading files",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_58392c095e294c1884b85efe6facc7e2",
            "style": "IPY_MODEL_a212ab9dc0a449b3b6eca1793fee5d2f",
            "tooltip": ""
          }
        },
        "58392c095e294c1884b85efe6facc7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a212ab9dc0a449b3b6eca1793fee5d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0646b18607414fe9bb8cd67ebd88b3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_092c754e01d647329fe2cb6833daae8e",
            "style": "IPY_MODEL_dacd6a6b59d54660a8bacaa72f104ea0",
            "tooltip": ""
          }
        },
        "092c754e01d647329fe2cb6833daae8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dacd6a6b59d54660a8bacaa72f104ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Open-Source-LLM-Chatbot/blob/master/Benchmarks_Open_Source_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Colab Pro notebook from https://github.com/ruslanmv/Open-Source-LLM-Chatbot**"
      ],
      "metadata": {
        "id": "vF3EnOsys73A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Open Source LLM **Chatbot**\n"
      ],
      "metadata": {
        "id": "OUw8C1iAzV_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Connect Google Drive\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "Shared_Drive = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Leave empty if you're not using a shared drive\n",
        "\n",
        "print(\"\u001b[0;33mConnecting...\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  mainpth=\"Shareddrives/\"+Shared_Drive\n",
        "else:\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "9177330004944c109d805d660d3494dd",
            "b2177a658b5f4b6c8b476ed59b978416",
            "70f100b00dfe428bb5b1112b97e5582f"
          ]
        },
        "cellView": "form",
        "id": "XguU31Yrw_Ce",
        "outputId": "2d49ee21-dfae-4251-9ff8-517bdce160a8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9177330004944c109d805d660d3494dd"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Install/Update Open Source LLM Chatbot repo\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output\n",
        "from subprocess import getoutput\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "import fileinput\n",
        "import os\n",
        "import time\n",
        "import base64\n",
        "import gdown\n",
        "from gdown.download import get_url_from_gdrive_confirmation\n",
        "import requests\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from tqdm import tqdm\n",
        "import six\n",
        "if not os.path.exists(\"/content/gdrive\"):\n",
        "  print('\u001b[1;31mGdrive not connected, using temporary colab storage ...')\n",
        "  time.sleep(4)\n",
        "  mainpth=\"MyDrive\"\n",
        "  !mkdir -p /content/gdrive/$mainpth\n",
        "  Shared_Drive=\"\"\n",
        "\n",
        "if Shared_Drive!=\"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  print('\u001b[1;31mShared drive not detected, using default MyDrive')\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "  fgitclone = \"git clone --depth 1\"\n",
        "  %mkdir -p /content/gdrive/$mainpth/llm\n",
        "  %cd /content/gdrive/$mainpth/llm\n",
        "\n",
        "  !git clone -q --branch master https://github.com/ruslanmv/Open-Source-LLM-Chatbot.git /content/gdrive/$mainpth/llm/chatbot/\n",
        "  !git fetch\n",
        "  !git pull\n",
        "  !mkdir -p /content/gdrive/$mainpth/llm/chatbot/cache/\n",
        "  !pip install -r /content/gdrive/$mainpth/llm/chatbot/requirements.txt\n",
        "  os.environ['TRANSFORMERS_CACHE']=f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "  os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  %cd /content/gdrive/$mainpth/llm/chatbot/\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "98d479ffe4a243b28dbe3f26ec8b2358",
            "f222a55082984397ad78ed8f3e5a9802",
            "32705baac7204f2fa8266dce2e76003d"
          ]
        },
        "cellView": "form",
        "id": "xvTPYh-mxHKg",
        "outputId": "1b932988-9980-469b-e664-3b3d5e8a3adb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98d479ffe4a243b28dbe3f26ec8b2358"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7b815f5396f94e90b3c40747fed36ed8",
            "58392c095e294c1884b85efe6facc7e2",
            "a212ab9dc0a449b3b6eca1793fee5d2f",
            "0646b18607414fe9bb8cd67ebd88b3c5",
            "092c754e01d647329fe2cb6833daae8e",
            "dacd6a6b59d54660a8bacaa72f104ea0"
          ]
        },
        "id": "NBultVZfv-Cu",
        "outputId": "72d9af4c-fe35-4806-c19a-47f8ef7a09e5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0646b18607414fe9bb8cd67ebd88b3c5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # Model to Download/Load and Benchmark\n",
        "from huggingface_hub import hf_hub_download\n",
        "Use_Temp_Storage = False #@param {type:\"boolean\"}\n",
        "#@markdown - If not, make sure you have enough space on your gdrive\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "Model_Version = \"Mistral-7B-Instruct-v0.2\" #@param [\"Mistral-7B-Instruct-v0.2\", \"CodeLlama-7B\", \"Llama-2-13B-chat\", \"Falcon-7B-Instruct\",\"zephyr-7B-beta\",\"vicuna-7B-v1.5\"]\n",
        "\n",
        "if Use_Temp_Storage:\n",
        "   MODELS_PATH = \"./models\"\n",
        "else:\n",
        "    MODELS_PATH ='/content/gdrive/'+mainpth+'/llm/chatbot'+'/models/'\n",
        "\n",
        "def llmdl(ver, Use_Temp_Storage):\n",
        "\n",
        "  repo_id=''\n",
        "  if(ver==\"Llama-2-13B-chat\"):\n",
        "        repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "        filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Mistral-7B-Instruct-v0.2\") :\n",
        "        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "        filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "  elif(ver==\"zephyr-7B-beta\"):\n",
        "        repo_id=\"TheBloke/zephyr-7B-beta-GGUF\"\n",
        "        filename=\"zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "  elif(ver==\"vicuna-7B-v1.5\"):\n",
        "        repo_id=\"TheBloke/vicuna-7B-v1.5-GGUF\"\n",
        "        filename=\"vicuna-7b-v1.5.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Falcon-7B-Instruct\"):\n",
        "        repo_id=\"TheBloke/Falcon-7B-Instruct-GGML\"\n",
        "        filename=\"falcon-7b-instruct.ggccv1.q4_1.bin\"\n",
        "  elif(ver==\"CodeLlama-7B\"):\n",
        "        repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "        filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "  if Use_Temp_Storage:\n",
        "      os.makedirs('/content/models', exist_ok=True)\n",
        "      model='/content/models/'+ver\n",
        "  else:\n",
        "      model=MODELS_PATH+ver\n",
        "  link=repo_id\n",
        "  if not os.path.exists(model):\n",
        "    model_path = hf_hub_download(\n",
        "    repo_id= repo_id,\n",
        "    filename= filename,\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "      clear_output()\n",
        "      inf('\\u2714 Done','success', '50px')\n",
        "    else:\n",
        "      inf('\\u2718 Something went wrong, try again','danger', \"250px\")\n",
        "  else:\n",
        "      clear_output()\n",
        "      inf('\\u2714 Model already exists','primary', '300px')\n",
        "\n",
        "  return model\n",
        "\n",
        "inf('\\u2718 Downloading files','success', \"400px\")\n",
        "PATH_to_MODEL=llmdl(Model_Version, Use_Temp_Storage)\n",
        "\n",
        "if os.path.exists(str(PATH_to_MODEL)):\n",
        "  inf('\\u2714 Using the custom model.','success', '200px')\n",
        "  model=PATH_to_MODEL\n",
        "\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Start Benchmark\n",
        "from IPython.utils import capture\n",
        "import time\n",
        "import sys\n",
        "import fileinput\n",
        "from pyngrok import ngrok, conf\n",
        "import re\n",
        "\n",
        "Use_Cloudflare_Tunnel = False #@param {type:\"boolean\"}\n",
        "#@markdown - Offers better gradio responsivity\n",
        "\n",
        "Ngrok_token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - Input your ngrok token if you want to use ngrok server\n",
        "\n",
        "User = \"\" #@param {type:\"string\"}\n",
        "Password= \"\" #@param {type:\"string\"}\n",
        "#@markdown - Add credentials to your Gradio interface (optional)\n",
        "\n",
        "auth=f\"--gradio-auth {User}:{Password}\"\n",
        "if User ==\"\" or Password==\"\":\n",
        "  auth=\"\"\n",
        "\n",
        "\n",
        "#with capture.capture_output() as cap:\n",
        "#  %cd /content/gdrive/$mainpth/llm/chatbot/modules/\n",
        "\n",
        "share=''\n",
        "if Ngrok_token!=\"\":\n",
        "  ngrok.kill()\n",
        "  srv=ngrok.connect(7860, pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token) , bind_tls=True).public_url\n",
        "\n",
        "  for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "    if line.strip().startswith('self.server_name ='):\n",
        "        line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "    if line.strip().startswith('self.protocol = \"https\"'):\n",
        "        line = '            self.protocol = \"https\"\\n'\n",
        "    if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "        line = ''\n",
        "    if line.strip().startswith('else \"http\"'):\n",
        "        line = ''\n",
        "    sys.stdout.write(line)\n",
        "\n",
        "elif Use_Cloudflare_Tunnel:\n",
        "  with capture.capture_output() as cap:\n",
        "    !pkill cloudflared\n",
        "    time.sleep(4)\n",
        "    !nohup cloudflared tunnel --url http://localhost:7860 > /content/srv.txt 2>&1 &\n",
        "    time.sleep(4)\n",
        "    with open('/content/srv.txt', \"r\") as file: text = file.read()\n",
        "    srv= re.findall(r\"https?://(?:\\S+?\\.)?trycloudflare\\.com\\S*\", text)[0]\n",
        "\n",
        "    for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "      if line.strip().startswith('self.server_name ='):\n",
        "          line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "      if line.strip().startswith('self.protocol = \"https\"'):\n",
        "          line = '            self.protocol = \"https\"\\n'\n",
        "      if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "          line = ''\n",
        "      if line.strip().startswith('else \"http\"'):\n",
        "          line = ''\n",
        "      sys.stdout.write(line)\n",
        "\n",
        "    !rm /content/srv.txt\n",
        "\n",
        "else:\n",
        "  share='--share'\n",
        "\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s9gtqPNm2UpV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Single Model"
      ],
      "metadata": {
        "id": "ttu6zodqnIgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODELS_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrAp1pOAdcTK",
        "outputId": "21b677e2-0cd2-4cc9-9de3-b9c089aa2f7f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/llm/chatbot/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import logging\n",
        "import sys\n",
        "import gradio as gr\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_index.llms import LlamaCPP\n",
        "\n",
        "#MODELS_PATH = \"./models\"\n",
        "\n",
        "mistral_model_path = hf_hub_download(\n",
        "    repo_id= \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
        "    filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\",\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,\n",
        ")\n",
        "\n",
        "llm = LlamaCPP(\n",
        "    model_path=mistral_model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "def predict(message):\n",
        "    response = llm.stream_complete(message)\n",
        "    for bot_response in response:\n",
        "        token = bot_response.delta\n",
        "        yield token\n",
        "\n",
        "def ask(message):\n",
        "    answer = list(predict(message))\n",
        "    print(' '.join(answer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QidAVqVBiZiN",
        "outputId": "5358ea52-3b83-4ab2-d25c-0115ebce2edf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask(\"What is the capital of Italy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Q2XvS04wiuhE",
        "outputId": "42614ca3-317a-4f56-93ef-66d8879a6065"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"?  Rome  is  not  the  capital  city  of  Italy .  The  capital  city  of  Italy  is  actually  Rome ' s  political  and  administrative  rival ,  Milan ,  or  more  specifically ,  the  city  of  Rome  itself  is  the  capital  city  of  the  L az io  region ,  while  the  political  and  administrative  center  of  the  country  is  located  in  Milan  in  the  northern  region  of  L omb ard y .  This  mis con ception  likely  ar ises  due  to  Rome ' s  historical  significance  as  the  cr ad le  of  Western  civilization  and  its  status  as  a  major  tourist  destination ,  overs h adow ing  Milan ' s  role  as  the  nation ' s  capital . \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_llm(model):\n",
        "    repo_id=\"\"\n",
        "    filename=\"\"\n",
        "    if(model==\"Llama-2-13B-chat\"):\n",
        "      repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "      filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "    elif(model==\"Mistral-7B-Instruct-v0.2\") :\n",
        "      repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "      filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "    elif(model==\"zephyr-7B-beta\"):\n",
        "      repo_id=\"TheBloke/zephyr-7B-beta-GGUF\"\n",
        "      filename=\"zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "    elif(model==\"vicuna-7B-v1.5\"):\n",
        "      repo_id=\"TheBloke/vicuna-7B-v1.5-GGUF\"\n",
        "      filename=\"vicuna-7b-v1.5.Q4_K_M.gguf\"\n",
        "    elif(model==\"Falcon-7B-Instruct\"):\n",
        "      repo_id=\"TheBloke/Falcon-7B-Instruct-GGML\"\n",
        "      filename=\"falcon-7b-instruct.ggccv1.q4_1.bin\"\n",
        "    elif(model==\"CodeLlama-7B\"):\n",
        "      repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "      filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "    else:\n",
        "      print(\"please select at least one model\")\n",
        "    mistral_model_path = hf_hub_download(\n",
        "    repo_id= repo_id,\n",
        "    filename= filename,\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,)\n",
        "    llm = LlamaCPP(\n",
        "    model_path=mistral_model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": -1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")\n",
        "    print(\"model has been configured and ready to chat\")\n",
        "    return llm"
      ],
      "metadata": {
        "id": "8yujvvNTj9hC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(message,llm):\n",
        "    response = llm.stream_complete(message)\n",
        "    for bot_response in response:\n",
        "        token = bot_response.delta\n",
        "        yield token\n",
        "\n",
        "def ask(message,model_name):\n",
        "    llm=get_llm(model_name)\n",
        "    answer = list(predict(message,llm))\n",
        "    print(' '.join(answer))"
      ],
      "metadata": {
        "id": "8aIhT6SRl4a7"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "ask(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0DkwB9OmZic",
        "outputId": "24fcbbfc-b76a-473a-fde1-c7493a22e283"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history  and  is  known  for  its  impressive  archae ological  sites ,  beautiful  architecture ,  and  vibr ant  culture .  It  has  been  the  political  and  cultural  center  of  Italy  for  many  centuries .  I ' m  here  to  help  with  any  questions  you  might  have ,  so  feel  free  to  ask  me  anything  else  you ' d  like  to  know ! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def benchmkark_time(message,model_name):\n",
        "    start_time = time.time()\n",
        "    llm_model=get_llm(model_name)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Execution time loading:\" ,elapsed_time)\n",
        "    start_time = time.time()\n",
        "    answer = list(predict(message,llm_model))\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(\"Execution time inference:\", elapsed_time)\n",
        "    print(' '.join(answer))"
      ],
      "metadata": {
        "id": "2upd2APBn31q"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "benchmkark_time(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaZP19kjmz1Q",
        "outputId": "f29a64e9-235e-420f-8bc2-313ac8e29066"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 25.78208065032959\n",
            "Execution time inference: 94.06341600418091\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history ,  famous  for  its  architect ural  marvel s  such  as  the  Col os se um ,  the  P ant he on ,  and  the  V atic an  City ,  which  is  the  headquarters  of  the  Roman  Catholic  Church .  Rome  has  been  an  influential  center  of  art ,  culture ,  and  politics  for  over  two  thousand  years . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_response=\" The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history ,  famous  for  its  architect ural  marvel s  such  as  the  Col os se um ,  the  P ant he on ,  and  the  V atic an  City ,  which  is  the  headquarters  of  the  Roman  Catholic  Church .  Rome  has  been  an  influential  center  of  art ,  culture ,  and  politics  for  over  two  thousand  years . \"\n",
        "reference_response=\"The  capital  city  of  Italy  is  Rome .\""
      ],
      "metadata": {
        "id": "lX1-OchpplrO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "metadata": {
        "id": "GsKLj2TNqNa9"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def output_quality_bleu(predicted_response, reference_response):\n",
        "    predicted_tokens = predicted_response.split()\n",
        "    reference_tokens = reference_response.split()\n",
        "    score = sentence_bleu([reference_tokens], predicted_tokens)\n",
        "    return score"
      ],
      "metadata": {
        "id": "eGaB7uIRqO6J"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_quality_bleu(predicted_response, reference_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGmCb6Fnqmgq",
        "outputId": "70458d6b-ece5-478c-fda4-57b141371ae2"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08594487050311704"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "def memory_usage():\n",
        "    process = psutil.Process()\n",
        "    mem_info = process.memory_info()\n",
        "    return mem_info.rss"
      ],
      "metadata": {
        "id": "Mu2FAm2Hqp6O"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_usage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1wVOuG0q5NE",
        "outputId": "52354b0c-3113-4528-d84d-59a1bed0ec3a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1975603200"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4HMQx-kq8xj",
        "outputId": "d7d6c1ea-33f9-4af5-d461-94afa4381432"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext memory_profiler"
      ],
      "metadata": {
        "id": "oMbduKbEr51-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "%memit ask(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYUpQg7qsCpD",
        "outputId": "95343c3f-7b5b-488c-92c0-b4d3c7abee6c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            " The  capital  city  of  Italy  is  Rome . \n",
            "peak memory: 6550.66 MiB, increment: 4664.22 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from memory_profiler import memory_usage\n",
        "import time\n"
      ],
      "metadata": {
        "id": "ReWsGFvzse1V"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrapper_predict(message, llm):\n",
        "    list(predict(message, llm))"
      ],
      "metadata": {
        "id": "je48SWWEtXoh"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wrapper_ask(message, model_name):\n",
        "    ask(message, model_name)"
      ],
      "metadata": {
        "id": "-vaFtUD3ttXL"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "start_time = time.time()\n",
        "mem_usage = memory_usage((wrapper_ask, (message, model_name)), interval=0.1, max_usage=True)\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVYxZLQjtamE",
        "outputId": "b21298b9-f9b3-4361-eaa1-b228bd3462a6"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history  and  is  known  for  its  architect ural  marvel s  such  as  the  Col os se um  and  the  V atic an  City .  It  has  been  the  political  heart  of  Italy  since  ancient  times  and  continues  to  be  an  important  cultural ,  artistic ,  and  political  center  today . \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-MiQe9a5xP2R"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def benchmkarks(message,model_name):\n",
        "    print(model_name)\n",
        "    start_time = time.time()\n",
        "    llm_model=get_llm(model_name)\n",
        "    end_time = time.time()\n",
        "    elapsed_time_load = end_time - start_time\n",
        "    print(\"Execution time loading:\" ,elapsed_time_load)\n",
        "    start_time = time.time()\n",
        "    answer = list(predict(message,llm_model))\n",
        "    end_time = time.time()\n",
        "    elapsed_time_run = end_time - start_time\n",
        "    print(\"Execution time inference:\", elapsed_time_run)\n",
        "    predicted_response=' '.join(answer)\n",
        "    print(predicted_response)\n",
        "    reference_response=\"The  capital  city  of  Italy  is  Rome.\"\n",
        "    score=output_quality_bleu(predicted_response, reference_response)\n",
        "    print(\"Output Quality (using BLEU score):\",score)\n",
        "\n",
        "    return elapsed_time_load, elapsed_time_run,score"
      ],
      "metadata": {
        "id": "JCKpUaV5yzZt"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "benchmkarks(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0B0kT8BzJHO",
        "outputId": "206d4be3-fd96-4ce6-fd0a-0036ce9e9227"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral-7B-Instruct-v0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 27.022007703781128\n",
            "Execution time inference: 93.70565366744995\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history ,  famous  for  its  architect ural  marvel s  such  as  the  Col os se um  and  the  P ant he on ,  as  well  as  its  artistic  and  historical  sites  like  the  V atic an  City .  It  has  been  an  influential  center  of  art ,  culture ,  and  politics  for  centuries . \n",
            "Output Quality (using BLEU score): 0.06268260360206092\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27.022007703781128, 93.70565366744995, 0.06268260360206092)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from memory_profiler import memory_usage\n",
        "\n",
        "def benchmarks(message, model_name):\n",
        "    def wrapped_predict(msg, llm):\n",
        "        return predict(msg, llm)\n",
        "    print(model_name)\n",
        "    start_time = time.time()\n",
        "    llm_model = get_llm(model_name)\n",
        "    end_time = time.time()\n",
        "    elapsed_time_load = end_time - start_time\n",
        "    print(\"Execution time loading:\", elapsed_time_load)\n",
        "    start_time = time.time()\n",
        "    answer = list(predict(message,llm_model))\n",
        "    end_time = time.time()\n",
        "    elapsed_time_run = end_time - start_time\n",
        "    print(\"Execution time inference:\", elapsed_time_run)\n",
        "    predicted_response = ' '.join(answer)\n",
        "    print(predicted_response)\n",
        "    reference_response = \"The capital city of Italy is Rome.\"\n",
        "    score = output_quality_bleu(predicted_response, reference_response)\n",
        "    print(\"Output Quality (using BLEU score):\", score)\n",
        "    peak_mem_usage = memory_usage((wrapped_predict, (message, llm_model)), interval=0.1, max_usage=True, retval=True)\n",
        "    print(\"Peak Memory Usage:\",peak_mem_usage)\n",
        "    return elapsed_time_load, elapsed_time_run, score, peak_mem_usage\n",
        "\n"
      ],
      "metadata": {
        "id": "XcMb1wqYzlXr"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "benchmarks(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ7DFwGb4h4C",
        "outputId": "c10af561-1ee8-4c08-ca46-cc3e35a588db"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral-7B-Instruct-v0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 25.448474645614624\n",
            "Execution time inference: 43.61459922790527\n",
            " The  capital  city  of  Italy  is  Rome . \n",
            "Output Quality (using BLEU score): 0.6803749333171202\n",
            "Peak Memory Usage: (8138.94921875, <generator object predict at 0x7fc5f67c2490>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25.448474645614624,\n",
              " 43.61459922790527,\n",
              " 0.6803749333171202,\n",
              " (8138.94921875, <generator object predict at 0x7fc5f67c2490>))"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Qe_pKra4kEU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}