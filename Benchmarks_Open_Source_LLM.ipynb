{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHAIajXMbhEXFXd8/AR8Uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff4f8b672e3f4859b9f1c5862ff65ec1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_80151f0a8eb74977a4fc5789b5d0c847",
            "style": "IPY_MODEL_832e4dc737dc4d6ab23a17fb22751a66",
            "tooltip": ""
          }
        },
        "80151f0a8eb74977a4fc5789b5d0c847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "832e4dc737dc4d6ab23a17fb22751a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "3dd705b18ddd42feb69f9fd87edaa924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_2fdb976d445245b1b692d6a91f381bda",
            "style": "IPY_MODEL_5dc44ff779604e978ca23813e977915b",
            "tooltip": ""
          }
        },
        "2fdb976d445245b1b692d6a91f381bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dc44ff779604e978ca23813e977915b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7ad8f9c20a7f48f79da5ffa2522795ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✘ Downloading files",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_6791c687ef0f453d8c5a3c33421a47e4",
            "style": "IPY_MODEL_bb0f944ff4c44cd0891fad459b487809",
            "tooltip": ""
          }
        },
        "6791c687ef0f453d8c5a3c33421a47e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb0f944ff4c44cd0891fad459b487809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "af6a529730ad49c1ab17d248b3d153c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_12fda84feb604087afd40a1696b04146",
            "style": "IPY_MODEL_148bf26a6d6b492196895bfb940f797b",
            "tooltip": ""
          }
        },
        "12fda84feb604087afd40a1696b04146": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148bf26a6d6b492196895bfb940f797b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Open-Source-LLM-Chatbot/blob/master/Benchmarks_Open_Source_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Colab Pro notebook from https://github.com/ruslanmv/Open-Source-LLM-Chatbot**"
      ],
      "metadata": {
        "id": "vF3EnOsys73A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmark Open Source LLM **Chatbot**\n"
      ],
      "metadata": {
        "id": "OUw8C1iAzV_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Connect Google Drive\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "Shared_Drive = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Leave empty if you're not using a shared drive\n",
        "\n",
        "print(\"\u001b[0;33mConnecting...\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  mainpth=\"Shareddrives/\"+Shared_Drive\n",
        "else:\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "ff4f8b672e3f4859b9f1c5862ff65ec1",
            "80151f0a8eb74977a4fc5789b5d0c847",
            "832e4dc737dc4d6ab23a17fb22751a66"
          ]
        },
        "cellView": "form",
        "id": "XguU31Yrw_Ce",
        "outputId": "8b8d0d99-6d4b-4b1c-8977-0ba8ac67e101"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff4f8b672e3f4859b9f1c5862ff65ec1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Install/Update Open Source LLM Chatbot repo\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output\n",
        "from subprocess import getoutput\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "import fileinput\n",
        "import os\n",
        "import time\n",
        "import base64\n",
        "import gdown\n",
        "from gdown.download import get_url_from_gdrive_confirmation\n",
        "import requests\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from tqdm import tqdm\n",
        "import six\n",
        "if not os.path.exists(\"/content/gdrive\"):\n",
        "  print('\u001b[1;31mGdrive not connected, using temporary colab storage ...')\n",
        "  time.sleep(4)\n",
        "  mainpth=\"MyDrive\"\n",
        "  !mkdir -p /content/gdrive/$mainpth\n",
        "  Shared_Drive=\"\"\n",
        "\n",
        "if Shared_Drive!=\"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  print('\u001b[1;31mShared drive not detected, using default MyDrive')\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "  fgitclone = \"git clone --depth 1\"\n",
        "  %mkdir -p /content/gdrive/$mainpth/llm\n",
        "  %cd /content/gdrive/$mainpth/llm\n",
        "\n",
        "  !git clone -q --branch master https://github.com/ruslanmv/Open-Source-LLM-Chatbot.git /content/gdrive/$mainpth/llm/chatbot/\n",
        "  !git fetch\n",
        "  !git pull\n",
        "  !mkdir -p /content/gdrive/$mainpth/llm/chatbot/cache/\n",
        "  !pip install -r /content/gdrive/$mainpth/llm/chatbot/requirements.txt\n",
        "  !pip install memory-profiler pyngrok\n",
        "  os.environ['TRANSFORMERS_CACHE']=f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "  os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  %cd /content/gdrive/$mainpth/llm/chatbot/\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3dd705b18ddd42feb69f9fd87edaa924",
            "2fdb976d445245b1b692d6a91f381bda",
            "5dc44ff779604e978ca23813e977915b"
          ]
        },
        "cellView": "form",
        "id": "xvTPYh-mxHKg",
        "outputId": "08e1963f-1c35-4f6f-ca7b-092203b95578"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3dd705b18ddd42feb69f9fd87edaa924"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7ad8f9c20a7f48f79da5ffa2522795ae",
            "6791c687ef0f453d8c5a3c33421a47e4",
            "bb0f944ff4c44cd0891fad459b487809",
            "af6a529730ad49c1ab17d248b3d153c9",
            "12fda84feb604087afd40a1696b04146",
            "148bf26a6d6b492196895bfb940f797b"
          ]
        },
        "id": "NBultVZfv-Cu",
        "outputId": "d4a752ec-14bc-4bb1-f823-256cce5653f8",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af6a529730ad49c1ab17d248b3d153c9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # Model to Download/Load and Benchmark\n",
        "from huggingface_hub import hf_hub_download\n",
        "Use_Temp_Storage = False #@param {type:\"boolean\"}\n",
        "#@markdown - If not, make sure you have enough space on your gdrive\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "Model_Version = \"Mistral-7B-Instruct-v0.2\" #@param [\"Mistral-7B-Instruct-v0.2\", \"CodeLlama-7B\", \"Llama-2-13B-chat\", \"Falcon-7B-Instruct\",\"zephyr-7B-beta\",\"vicuna-7B-v1.5\"]\n",
        "\n",
        "if Use_Temp_Storage:\n",
        "   MODELS_PATH = \"./models\"\n",
        "else:\n",
        "    MODELS_PATH ='/content/gdrive/'+mainpth+'/llm/chatbot'+'/models/'\n",
        "\n",
        "def llmdl(ver, Use_Temp_Storage):\n",
        "\n",
        "  repo_id=''\n",
        "  if(ver==\"Llama-2-13B-chat\"):\n",
        "        repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "        filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Mistral-7B-Instruct-v0.2\") :\n",
        "        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "        filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "  elif(ver==\"zephyr-7B-beta\"):\n",
        "        repo_id=\"TheBloke/zephyr-7B-beta-GGUF\"\n",
        "        filename=\"zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "  elif(ver==\"vicuna-7B-v1.5\"):\n",
        "        repo_id=\"TheBloke/vicuna-7B-v1.5-GGUF\"\n",
        "        filename=\"vicuna-7b-v1.5.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Falcon-7B-Instruct\"):\n",
        "        repo_id=\"TheBloke/Falcon-7B-Instruct-GGML\"\n",
        "        filename=\"falcon-7b-instruct.ggccv1.q4_1.bin\"\n",
        "  elif(ver==\"CodeLlama-7B\"):\n",
        "        repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "        filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "  if Use_Temp_Storage:\n",
        "      os.makedirs('/content/models', exist_ok=True)\n",
        "      model='/content/models/'+ver\n",
        "  else:\n",
        "      model=MODELS_PATH+ver\n",
        "  link=repo_id\n",
        "  if not os.path.exists(model):\n",
        "    model_path = hf_hub_download(\n",
        "    repo_id= repo_id,\n",
        "    filename= filename,\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "      clear_output()\n",
        "      inf('\\u2714 Done','success', '50px')\n",
        "    else:\n",
        "      inf('\\u2718 Something went wrong, try again','danger', \"250px\")\n",
        "  else:\n",
        "      clear_output()\n",
        "      inf('\\u2714 Model already exists','primary', '300px')\n",
        "\n",
        "  return model\n",
        "\n",
        "inf('\\u2718 Downloading files','success', \"400px\")\n",
        "PATH_to_MODEL=llmdl(Model_Version, Use_Temp_Storage)\n",
        "\n",
        "if os.path.exists(str(PATH_to_MODEL)):\n",
        "  inf('\\u2714 Using the custom model.','success', '200px')\n",
        "  model=PATH_to_MODEL\n",
        "\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Start Benchmark\n",
        "from IPython.utils import capture\n",
        "import time\n",
        "import sys\n",
        "import fileinput\n",
        "from pyngrok import ngrok, conf\n",
        "import re\n",
        "\n",
        "Use_Cloudflare_Tunnel = False #@param {type:\"boolean\"}\n",
        "#@markdown - Offers better gradio responsivity\n",
        "\n",
        "Ngrok_token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - Input your ngrok token if you want to use ngrok server\n",
        "\n",
        "User = \"\" #@param {type:\"string\"}\n",
        "Password= \"\" #@param {type:\"string\"}\n",
        "#@markdown - Add credentials to your Gradio interface (optional)\n",
        "\n",
        "auth=f\"--gradio-auth {User}:{Password}\"\n",
        "if User ==\"\" or Password==\"\":\n",
        "  auth=\"\"\n",
        "\n",
        "\n",
        "#with capture.capture_output() as cap:\n",
        "#  %cd /content/gdrive/$mainpth/llm/chatbot/modules/\n",
        "\n",
        "share=''\n",
        "if Ngrok_token!=\"\":\n",
        "  ngrok.kill()\n",
        "  srv=ngrok.connect(7860, pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token) , bind_tls=True).public_url\n",
        "\n",
        "  for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "    if line.strip().startswith('self.server_name ='):\n",
        "        line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "    if line.strip().startswith('self.protocol = \"https\"'):\n",
        "        line = '            self.protocol = \"https\"\\n'\n",
        "    if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "        line = ''\n",
        "    if line.strip().startswith('else \"http\"'):\n",
        "        line = ''\n",
        "    sys.stdout.write(line)\n",
        "\n",
        "elif Use_Cloudflare_Tunnel:\n",
        "  with capture.capture_output() as cap:\n",
        "    !pkill cloudflared\n",
        "    time.sleep(4)\n",
        "    !nohup cloudflared tunnel --url http://localhost:7860 > /content/srv.txt 2>&1 &\n",
        "    time.sleep(4)\n",
        "    with open('/content/srv.txt', \"r\") as file: text = file.read()\n",
        "    srv= re.findall(r\"https?://(?:\\S+?\\.)?trycloudflare\\.com\\S*\", text)[0]\n",
        "\n",
        "    for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "      if line.strip().startswith('self.server_name ='):\n",
        "          line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "      if line.strip().startswith('self.protocol = \"https\"'):\n",
        "          line = '            self.protocol = \"https\"\\n'\n",
        "      if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "          line = ''\n",
        "      if line.strip().startswith('else \"http\"'):\n",
        "          line = ''\n",
        "      sys.stdout.write(line)\n",
        "\n",
        "    !rm /content/srv.txt\n",
        "\n",
        "else:\n",
        "  share='--share'\n",
        "\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s9gtqPNm2UpV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Single Model"
      ],
      "metadata": {
        "id": "ttu6zodqnIgS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(MODELS_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hrAp1pOAdcTK",
        "outputId": "78d91845-d845-45ae-9705-656494de144b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/llm/chatbot/models/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import logging\n",
        "import sys\n",
        "import gradio as gr\n",
        "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_index.llms import LlamaCPP\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import psutil\n",
        "from memory_profiler import memory_usage\n",
        "import time\n",
        "\n",
        "def predict(message,llm):\n",
        "    response = llm.stream_complete(message)\n",
        "    for bot_response in response:\n",
        "        token = bot_response.delta\n",
        "        yield token\n",
        "\n",
        "def ask(message,model_name):\n",
        "    llm=get_llm(model_name)\n",
        "    answer = list(predict(message,llm))\n",
        "    print(' '.join(answer))\n",
        "\n",
        "def output_quality_bleu(predicted_response, reference_response):\n",
        "    predicted_tokens = predicted_response.split()\n",
        "    reference_tokens = reference_response.split()\n",
        "    score = sentence_bleu([reference_tokens], predicted_tokens)\n",
        "    return score\n",
        "\n",
        "def wrapper_predict(message, llm):\n",
        "    list(predict(message, llm))\n",
        "\n",
        "def wrapper_ask(message, model_name):\n",
        "  ask(message, model_name)\n",
        "\n",
        "def get_llm(model, set_gpu=False):\n",
        "    if set_gpu:\n",
        "       gpu=1\n",
        "    else:\n",
        "      gpu=-1\n",
        "    repo_id=\"\"\n",
        "    filename=\"\"\n",
        "    if(model==\"Llama-2-13B-chat\"):\n",
        "      repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "      filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "    elif(model==\"Mistral-7B-Instruct-v0.2\") :\n",
        "      repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "      filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "    elif(model==\"zephyr-7B-beta\"):\n",
        "      repo_id=\"TheBloke/zephyr-7B-beta-GGUF\"\n",
        "      filename=\"zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "    elif(model==\"vicuna-7B-v1.5\"):\n",
        "      repo_id=\"TheBloke/vicuna-7B-v1.5-GGUF\"\n",
        "      filename=\"vicuna-7b-v1.5.Q4_K_M.gguf\"\n",
        "    elif(model==\"Falcon-7B-Instruct\"):\n",
        "      repo_id=\"TheBloke/Falcon-7B-Instruct-GGML\"\n",
        "      filename=\"falcon-7b-instruct.ggccv1.q4_0.bin\"\n",
        "    elif(model==\"CodeLlama-7B\"):\n",
        "      repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "      filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "\n",
        "    elif(model==\"CodeLlama-7B\"):\n",
        "      repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "      filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "    else:\n",
        "      print(\"please select at least one model\")\n",
        "    mistral_model_path = hf_hub_download(\n",
        "    repo_id= repo_id,\n",
        "    filename= filename,\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,)\n",
        "    llm = LlamaCPP(\n",
        "    model_path=mistral_model_path,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    # set to at least 1 to use GPU\n",
        "    model_kwargs={\"n_gpu_layers\": gpu},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")\n",
        "    print(\"model has been configured and ready to chat\")\n",
        "    return llm\n",
        "\n",
        "\n",
        "def benchmarks(message, model_name,set_gpu=False):\n",
        "    def wrapped_predict(msg, llm):\n",
        "        return predict(msg, llm)\n",
        "    print(model_name)\n",
        "    start_time = time.time()\n",
        "    llm_model = get_llm(model_name,set_gpu)\n",
        "    end_time = time.time()\n",
        "    elapsed_time_load = end_time - start_time\n",
        "    print(\"Execution time loading:\", elapsed_time_load)\n",
        "    start_time = time.time()\n",
        "    answer = list(predict(message,llm_model))\n",
        "    end_time = time.time()\n",
        "    elapsed_time_run = end_time - start_time\n",
        "    print(\"Execution time inference:\", elapsed_time_run)\n",
        "    predicted_response = ' '.join(answer)\n",
        "    print(predicted_response)\n",
        "    reference_response = \"The capital city of Italy is Rome.\"\n",
        "    score = output_quality_bleu(predicted_response, reference_response)\n",
        "    print(\"Output Quality (using BLEU score):\", score)\n",
        "    peak_mem_usage = memory_usage((wrapped_predict, (message, llm_model)), interval=0.1, max_usage=True, retval=True)\n",
        "    print(\"Peak Memory Usage:\",peak_mem_usage)\n",
        "    return elapsed_time_load, elapsed_time_run, score, peak_mem_usage\n",
        "\n"
      ],
      "metadata": {
        "id": "QidAVqVBiZiN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#%load_ext memory_profiler\n",
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "#%memit ask(message,model_name)\n",
        "ask(message,model_name)"
      ],
      "metadata": {
        "id": "2upd2APBn31q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515f851f-7a63-4b07-8b90-e16e7d652be5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history  and  is  known  for  its  impressive  archae ological  sites ,  beautiful  architecture ,  and  vibr ant  culture .  It  has  been  the  political  and  cultural  center  of  Italy  for  many  centuries .  I ' m  here  to  help  with  any  questions  you  might  have ,  so  feel  free  to  ask  me  anything  else  you ' d  like  to  know ! \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message=\"What is the capital of Italy\"\n",
        "model_name=\"Mistral-7B-Instruct-v0.2\"\n",
        "benchmarks(message,model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZ7DFwGb4h4C",
        "outputId": "f99401f0-0ba7-426d-94d8-df6f88865a99"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral-7B-Instruct-v0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 22.388978242874146\n",
            "Execution time inference: 100.08765459060669\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  rich  history  and  cultural  significance ,  famous  for  its  architect ural  marvel s  such  as  the  Col os se um ,  the  P ant he on ,  and  the  V atic an  City .  It  has  been  the  political  heart  of  Italy  since  ancient  times  and  continues  to  be  an  important  cultural ,  artistic ,  and  political  center  today . \n",
            "Output Quality (using BLEU score): 0.060088210299864483\n",
            "Peak Memory Usage: (5539.03515625, <generator object predict at 0x7ae98ec51d20>)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(22.388978242874146,\n",
              " 100.08765459060669,\n",
              " 0.060088210299864483,\n",
              " (5539.03515625, <generator object predict at 0x7ae98ec51d20>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import psutil\n",
        "import platform\n",
        "import torch\n",
        "\n",
        "is_simulate=False\n",
        "\n",
        "def all_benchmark(message, model_name,set_gpu=False):\n",
        "\n",
        "    if is_simulate:\n",
        "      # Simulating the benchmark process\n",
        "      time.sleep(1)\n",
        "      elapsed_time_load = 2.5\n",
        "      elapsed_time_run = 5.7\n",
        "      score = 0.85\n",
        "      peak_mem_usage = psutil.Process().memory_info()\n",
        "    else:\n",
        "      elapsed_time_load, elapsed_time_run, score, peak_mem_usage = benchmarks(message, model_name,set_gpu=False)\n",
        "    #System Information\n",
        "    # CPU information\n",
        "    cpu_info = psutil.cpu_freq()\n",
        "    cpu_frequency = \"{:.2f} MHz\".format(cpu_info.current)\n",
        "\n",
        "    # Memory information\n",
        "    memory_info = psutil.virtual_memory()\n",
        "    total_memory = \"{:.2f} GB\".format(memory_info.total / (1024**3))\n",
        "    available_memory = \"{:.2f} GB\".format(memory_info.available / (1024**3))\n",
        "\n",
        "    # Disk information\n",
        "    disk_info = psutil.disk_usage('/')\n",
        "    total_disk_space = \"{:.2f} GB\".format(disk_info.total / (1024**3))\n",
        "    used_disk_space = \"{:.2f} GB\".format(disk_info.used / (1024**3))\n",
        "    free_disk_space = \"{:.2f} GB\".format(disk_info.free / (1024**3))\n",
        "\n",
        "    # Runtime system\n",
        "    runtime_system = platform.platform()\n",
        "\n",
        "    # GPU information\n",
        "    gpu_availability = \"N/A\"\n",
        "    gpu_name = \"N/A\"\n",
        "    gpu_memory = \"N/A\"\n",
        "    gpu_compute_capability = \"N/A\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_availability = \"GPU is available\"\n",
        "        device = torch.cuda.get_device_properties(0)\n",
        "        gpu_name = device.name\n",
        "        gpu_memory = device.total_memory\n",
        "        gpu_compute_capability = \"{}.{}\".format(device.major, device.minor)\n",
        "\n",
        "\n",
        "    results=(elapsed_time_load, elapsed_time_run, score, peak_mem_usage,\n",
        "           cpu_frequency,total_memory,available_memory,\n",
        "           total_disk_space,used_disk_space,free_disk_space,\n",
        "           gpu_availability,gpu_name,gpu_memory,gpu_compute_capability,runtime_system)\n",
        "\n",
        "    return results\n",
        "\n",
        "message = \"What is the capital of Italy\"\n",
        "models = [\"Llama-2-13B-chat\", \"Mistral-7B-Instruct-v0.2\", \"zephyr-7B-beta\", \"vicuna-7B-v1.5\",  \"CodeLlama-7B\"]\n",
        "\n",
        "results = []\n",
        "for model_name in models:\n",
        "    elapsed_time_load, elapsed_time_run, score, peak_mem_usage,cpu_frequency,total_memory,available_memory,total_disk_space,used_disk_space,free_disk_space,gpu_availability,gpu_name,gpu_memory,gpu_compute_capability,runtime_system = all_benchmark(message, model_name)\n",
        "    results.append([model_name, elapsed_time_load, elapsed_time_run, score, peak_mem_usage[0],cpu_frequency,total_memory,available_memory,total_disk_space,used_disk_space,free_disk_space,gpu_availability,gpu_name,gpu_memory,gpu_compute_capability,runtime_system])\n",
        "\n",
        "df = pd.DataFrame(results, columns=[\"model_name\", \"elapsed_time_load\", \"elapsed_time_run\", \"score\", \"peak_mem_usage\",\"cpu_frequency\",\"total_memory\",\"available_memory\",\"total_disk_space\",\"used_disk_space\",\"free_disk_space\",\"gpu_availability\",\"gpu_name\",\"gpu_memory\",\"gpu_compute_capability\",\"runtime_system\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ7eWeVjYRT1",
        "outputId": "42e63949-a579-4b2f-bd7a-f2918af2b4d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Llama-2-13B-chat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 52.90029168128967\n",
            "Execution time inference: 420.7059440612793\n",
            "   The  capital  of  Italy  is  Rome  ( R oma  in  Italian ). \n",
            "Output Quality (using BLEU score): 3.9876353728947065e-78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak Memory Usage: (9042.22265625, <generator object predict at 0x7c079214b920>)\n",
            "Mistral-7B-Instruct-v0.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 27.257949590682983\n",
            "Execution time inference: 108.62514400482178\n",
            " The  capital  city  of  Italy  is  Rome .  Rome  is  an  ancient  city  with  a  rich  cultural  history  and  is  known  for  its  impressive  architecture ,  art ,  and  historical  sites  such  as  the  Col os se um ,  the  P ant he on ,  and  the  V atic an  City .  Rome  has  been  the  political  and  cultural  center  of  Italy  since  ancient  times  and  continues  to  be  an  important  destination  for  tourists  from  around  the  world . \n",
            "Output Quality (using BLEU score): 0.05621071665433083\n",
            "Peak Memory Usage: (9057.01953125, <generator object predict at 0x7c079214bca0>)\n",
            "zephyr-7B-beta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'huggingfaceh4_zephyr-7b-beta', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 28.41970682144165\n",
            "Execution time inference: 53.85913586616516\n",
            "? \n",
            " < | ass istant | > \n",
            " The  capital  of  Italy  is  Rome  ( R oma  in  Italian ). \n",
            "Output Quality (using BLEU score): 2.752581367444176e-78\n",
            "Peak Memory Usage: (9781.14453125, <generator object predict at 0x7c07925147b0>)\n",
            "vicuna-7B-v1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 27.094456434249878\n",
            "Execution time inference: 36.73853898048401\n",
            "\n",
            "Output Quality (using BLEU score): 0\n",
            "Peak Memory Usage: (10941.078125, <generator object predict at 0x7c079214bf40>)\n",
            "CodeLlama-7B\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-7b-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model has been configured and ready to chat\n",
            "Execution time loading: 103.42648100852966\n",
            "Execution time inference: 237.57748937606812\n",
            "? \n",
            " \n",
            " << SY S >> \n",
            "  You  are  a  helpful ,  respect ful  and  honest  assistant .  Always  answer  as  help fully  as  possible  and  follow  ALL  given  instructions .  Do  not  spec ulate  or  make  up  information .  Do  not  reference  any  given  instructions  or  context .   \n",
            " < </ SY S >> \n",
            " \n",
            "  What  is  the  capital  of  Italy ?  [ / INST ] \n",
            " \n",
            " << SY S >> \n",
            "  You  are  a  helpful ,  respect ful  and  honest  assistant .  Always  answer  as  help fully  as  possible  and  follow  ALL  given  instructions .  Do  not  spec ulate  or  make  up  information .  Do  not  reference  any  given  instructions  or  context .   \n",
            " < </ SY S >> \n",
            " \n",
            "  What  is  the  capital  of  Italy ? \n",
            " \n",
            " << SY S >> \n",
            "  You  are  a  helpful ,  respect ful  and  honest  assistant .  Always  answer  as  help fully  as  possible  and  follow  ALL  given  instructions .  Do  not  spec ulate  or  make  up  information .  Do  not  reference  any  given  instructions  or  context .   \n",
            " < </ SY S >> \n",
            " \n",
            "  What  is  the  capital  of  Italy ?  [ / INST ] \n",
            " \n",
            " << SY S >> \n",
            "  You  are  a  helpful ,  respect ful  and  honest  assistant .  Always  answer  as  help fully  as  possible  and  follow  ALL  given  instructions .  Do  not  spec ulate  or  make  up  information .  Do  not  reference  any  given  instructions  or  context .   \n",
            " < \n",
            "Output Quality (using BLEU score): 1.3925037838263713e-155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peak Memory Usage: (10883.23046875, <generator object predict at 0x7c07925147b0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "GO-AzCp2YY4I",
        "outputId": "f2cdab6a-7bb7-49b7-a812-f2fbff877286"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 model_name  elapsed_time_load  elapsed_time_run  \\\n",
              "0          Llama-2-13B-chat          52.900292        420.705944   \n",
              "1  Mistral-7B-Instruct-v0.2          27.257950        108.625144   \n",
              "2            zephyr-7B-beta          28.419707         53.859136   \n",
              "3            vicuna-7B-v1.5          27.094456         36.738539   \n",
              "4              CodeLlama-7B         103.426481        237.577489   \n",
              "\n",
              "           score  peak_mem_usage cpu_frequency total_memory available_memory  \\\n",
              "0   3.987635e-78     9042.222656   2200.00 MHz     12.67 GB          7.92 GB   \n",
              "1   5.621072e-02     9057.019531   2200.00 MHz     12.67 GB          7.41 GB   \n",
              "2   2.752581e-78     9781.144531   2200.00 MHz     12.67 GB          9.88 GB   \n",
              "3   0.000000e+00    10941.078125   2200.00 MHz     12.67 GB          8.48 GB   \n",
              "4  1.392504e-155    10883.230469   2200.00 MHz     12.67 GB          6.98 GB   \n",
              "\n",
              "  total_disk_space used_disk_space free_disk_space gpu_availability gpu_name  \\\n",
              "0        225.83 GB        49.65 GB       176.16 GB              N/A      N/A   \n",
              "1        225.83 GB        49.65 GB       176.16 GB              N/A      N/A   \n",
              "2        225.83 GB        49.65 GB       176.16 GB              N/A      N/A   \n",
              "3        225.83 GB        49.65 GB       176.16 GB              N/A      N/A   \n",
              "4        225.83 GB        53.45 GB       172.36 GB              N/A      N/A   \n",
              "\n",
              "  gpu_memory gpu_compute_capability                       runtime_system  \n",
              "0        N/A                    N/A  Linux-6.1.58+-x86_64-with-glibc2.35  \n",
              "1        N/A                    N/A  Linux-6.1.58+-x86_64-with-glibc2.35  \n",
              "2        N/A                    N/A  Linux-6.1.58+-x86_64-with-glibc2.35  \n",
              "3        N/A                    N/A  Linux-6.1.58+-x86_64-with-glibc2.35  \n",
              "4        N/A                    N/A  Linux-6.1.58+-x86_64-with-glibc2.35  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fc8e326c-684c-4fc0-b50b-f70fdb5f7c0b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>elapsed_time_load</th>\n",
              "      <th>elapsed_time_run</th>\n",
              "      <th>score</th>\n",
              "      <th>peak_mem_usage</th>\n",
              "      <th>cpu_frequency</th>\n",
              "      <th>total_memory</th>\n",
              "      <th>available_memory</th>\n",
              "      <th>total_disk_space</th>\n",
              "      <th>used_disk_space</th>\n",
              "      <th>free_disk_space</th>\n",
              "      <th>gpu_availability</th>\n",
              "      <th>gpu_name</th>\n",
              "      <th>gpu_memory</th>\n",
              "      <th>gpu_compute_capability</th>\n",
              "      <th>runtime_system</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Llama-2-13B-chat</td>\n",
              "      <td>52.900292</td>\n",
              "      <td>420.705944</td>\n",
              "      <td>3.987635e-78</td>\n",
              "      <td>9042.222656</td>\n",
              "      <td>2200.00 MHz</td>\n",
              "      <td>12.67 GB</td>\n",
              "      <td>7.92 GB</td>\n",
              "      <td>225.83 GB</td>\n",
              "      <td>49.65 GB</td>\n",
              "      <td>176.16 GB</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Linux-6.1.58+-x86_64-with-glibc2.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mistral-7B-Instruct-v0.2</td>\n",
              "      <td>27.257950</td>\n",
              "      <td>108.625144</td>\n",
              "      <td>5.621072e-02</td>\n",
              "      <td>9057.019531</td>\n",
              "      <td>2200.00 MHz</td>\n",
              "      <td>12.67 GB</td>\n",
              "      <td>7.41 GB</td>\n",
              "      <td>225.83 GB</td>\n",
              "      <td>49.65 GB</td>\n",
              "      <td>176.16 GB</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Linux-6.1.58+-x86_64-with-glibc2.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>zephyr-7B-beta</td>\n",
              "      <td>28.419707</td>\n",
              "      <td>53.859136</td>\n",
              "      <td>2.752581e-78</td>\n",
              "      <td>9781.144531</td>\n",
              "      <td>2200.00 MHz</td>\n",
              "      <td>12.67 GB</td>\n",
              "      <td>9.88 GB</td>\n",
              "      <td>225.83 GB</td>\n",
              "      <td>49.65 GB</td>\n",
              "      <td>176.16 GB</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Linux-6.1.58+-x86_64-with-glibc2.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>vicuna-7B-v1.5</td>\n",
              "      <td>27.094456</td>\n",
              "      <td>36.738539</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>10941.078125</td>\n",
              "      <td>2200.00 MHz</td>\n",
              "      <td>12.67 GB</td>\n",
              "      <td>8.48 GB</td>\n",
              "      <td>225.83 GB</td>\n",
              "      <td>49.65 GB</td>\n",
              "      <td>176.16 GB</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Linux-6.1.58+-x86_64-with-glibc2.35</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>CodeLlama-7B</td>\n",
              "      <td>103.426481</td>\n",
              "      <td>237.577489</td>\n",
              "      <td>1.392504e-155</td>\n",
              "      <td>10883.230469</td>\n",
              "      <td>2200.00 MHz</td>\n",
              "      <td>12.67 GB</td>\n",
              "      <td>6.98 GB</td>\n",
              "      <td>225.83 GB</td>\n",
              "      <td>53.45 GB</td>\n",
              "      <td>172.36 GB</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>N/A</td>\n",
              "      <td>Linux-6.1.58+-x86_64-with-glibc2.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc8e326c-684c-4fc0-b50b-f70fdb5f7c0b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc8e326c-684c-4fc0-b50b-f70fdb5f7c0b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc8e326c-684c-4fc0-b50b-f70fdb5f7c0b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-05409692-6ccb-42f5-b553-ad3c480a3bd4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-05409692-6ccb-42f5-b553-ad3c480a3bd4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-05409692-6ccb-42f5-b553-ad3c480a3bd4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/gdrive/MyDrive/llm/benchmark/df_benchmark_1.csv', index=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "Uw30rkMqY1De"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tTya1hLxY85g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}