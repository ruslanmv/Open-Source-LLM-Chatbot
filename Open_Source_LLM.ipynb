{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6PacNdKz56mmdujohNZig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b48e246998c648c296714e641cbe5c40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_7bffa23a7a714ff3a99ee25f9cff98bc",
            "style": "IPY_MODEL_9dcd24d742a14283aa03b73f93d419d6",
            "tooltip": ""
          }
        },
        "7bffa23a7a714ff3a99ee25f9cff98bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dcd24d742a14283aa03b73f93d419d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "76982ce7d5674a67bf8a0e6e5bb8b433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✘ Downloading files",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_604464fb38bd4fcc898548ee2497ddcc",
            "style": "IPY_MODEL_01ab4b00657a40e88311c3226ed43aaf",
            "tooltip": ""
          }
        },
        "604464fb38bd4fcc898548ee2497ddcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "400px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01ab4b00657a40e88311c3226ed43aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "c9697961b1a8420ba2794c5f4511ca51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "✔ Done",
            "disabled": true,
            "icon": "",
            "layout": "IPY_MODEL_648ffe0c82ca424c857d5097cd337124",
            "style": "IPY_MODEL_37cfb76a700342c8966366ab9b1d1715",
            "tooltip": ""
          }
        },
        "648ffe0c82ca424c857d5097cd337124": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": "50px",
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37cfb76a700342c8966366ab9b1d1715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Open-Source-LLM-Chatbot/blob/master/Open_Source_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Colab Pro notebook from https://github.com/ruslanmv/Open-Source-LLM-Chatbot**"
      ],
      "metadata": {
        "id": "vF3EnOsys73A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Open Source LLM **Chatbot**\n"
      ],
      "metadata": {
        "id": "OUw8C1iAzV_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Connect Google Drive\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "Shared_Drive = \"\" #@param {type:\"string\"}\n",
        "#@markdown - Leave empty if you're not using a shared drive\n",
        "\n",
        "print(\"\u001b[0;33mConnecting...\")\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if Shared_Drive!=\"\" and os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  mainpth=\"Shareddrives/\"+Shared_Drive\n",
        "else:\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XguU31Yrw_Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Install/Update Open Source LLM Chatbot repo\n",
        "from IPython.utils import capture\n",
        "from IPython.display import clear_output\n",
        "from subprocess import getoutput\n",
        "import ipywidgets as widgets\n",
        "import sys\n",
        "import fileinput\n",
        "import os\n",
        "import time\n",
        "import base64\n",
        "import gdown\n",
        "from gdown.download import get_url_from_gdrive_confirmation\n",
        "import requests\n",
        "from urllib.request import urlopen, Request\n",
        "from urllib.parse import urlparse, parse_qs, unquote\n",
        "from tqdm import tqdm\n",
        "import six\n",
        "if not os.path.exists(\"/content/gdrive\"):\n",
        "  print('\u001b[1;31mGdrive not connected, using temporary colab storage ...')\n",
        "  time.sleep(4)\n",
        "  mainpth=\"MyDrive\"\n",
        "  !mkdir -p /content/gdrive/$mainpth\n",
        "  Shared_Drive=\"\"\n",
        "\n",
        "if Shared_Drive!=\"\" and not os.path.exists(\"/content/gdrive/Shareddrives\"):\n",
        "  print('\u001b[1;31mShared drive not detected, using default MyDrive')\n",
        "  mainpth=\"MyDrive\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  def inf(msg, style, wdth): inf = widgets.Button(description=msg, disabled=True, button_style=style, layout=widgets.Layout(min_width=wdth));display(inf)\n",
        "  fgitclone = \"git clone --depth 1\"\n",
        "  %mkdir -p /content/gdrive/$mainpth/llm\n",
        "  %cd /content/gdrive/$mainpth/llm\n",
        "\n",
        "  !git clone -q --branch master https://github.com/ruslanmv/Open-Source-LLM-Chatbot.git /content/gdrive/$mainpth/llm/chatbot/\n",
        "  !git fetch\n",
        "  !git pull\n",
        "  !mkdir -p /content/gdrive/$mainpth/llm/chatbot/cache/\n",
        "  !pip install -r /content/gdrive/$mainpth/llm/chatbot/requirements.txt\n",
        "  os.environ['TRANSFORMERS_CACHE']=f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "  os.environ['TORCH_HOME'] = f\"/content/gdrive/{mainpth}/llm/chatbot/cache\"\n",
        "\n",
        "with capture.capture_output() as cap:\n",
        "  %cd /content/gdrive/$mainpth/llm/chatbot/\n",
        "clear_output()\n",
        "inf('\\u2714 Done','success', '50px')\n",
        "\n",
        "#@markdown ---"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b48e246998c648c296714e641cbe5c40",
            "7bffa23a7a714ff3a99ee25f9cff98bc",
            "9dcd24d742a14283aa03b73f93d419d6"
          ]
        },
        "cellView": "form",
        "id": "xvTPYh-mxHKg",
        "outputId": "6cba4ea9-8b27-46fc-efda-f24030924a90"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b48e246998c648c296714e641cbe5c40"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "U7fB_MksnnKA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "76982ce7d5674a67bf8a0e6e5bb8b433",
            "604464fb38bd4fcc898548ee2497ddcc",
            "01ab4b00657a40e88311c3226ed43aaf",
            "c9697961b1a8420ba2794c5f4511ca51",
            "648ffe0c82ca424c857d5097cd337124",
            "37cfb76a700342c8966366ab9b1d1715"
          ]
        },
        "id": "NBultVZfv-Cu",
        "outputId": "7f98e479-9dc1-48d0-9aab-6bf05f2e01b9",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(button_style='success', description='✔ Done', disabled=True, layout=Layout(min_width='50px'), style=But…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9697961b1a8420ba2794c5f4511ca51"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown # Model Download/Load\n",
        "from huggingface_hub import hf_hub_download\n",
        "Use_Temp_Storage = False #@param {type:\"boolean\"}\n",
        "#@markdown - If not, make sure you have enough space on your gdrive\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "Model_Version = \"Mistral-7B-Instruct-v0.2\" #@param [\"Mistral-7B-Instruct-v0.2\", \"CodeLlama-7B\", \"Llama-2-13B-chat\", \"Falcon-7B-Instruct\",\"zephyr-7B-beta\",\"vicuna-7B-v1.5\"]\n",
        "\n",
        "if Use_Temp_Storage:\n",
        "   MODELS_PATH = \"./models\"\n",
        "else:\n",
        "    MODELS_PATH ='/content/gdrive/'+mainpth+'/llm/chatbot'+'/models/'\n",
        "\n",
        "def llmdl(ver, Use_Temp_Storage):\n",
        "\n",
        "  repo_id=''\n",
        "  if(ver==\"Llama-2-13B-chat\"):\n",
        "        repo_id=\"TheBloke/Llama-2-13B-chat-GGUF\"\n",
        "        filename=\"llama-2-13b-chat.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Mistral-7B-Instruct-v0.2\") :\n",
        "        repo_id=\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "        filename=\"mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "  elif(ver==\"zephyr-7B-beta\"):\n",
        "        repo_id=\"TheBloke/zephyr-7B-beta-GGUF \"\n",
        "        filename=\"zephyr-7b-beta.Q4_K_M.gguf\"\n",
        "  elif(ver==\"vicuna-7B-v1.5\"):\n",
        "        repo_id=\"TheBloke/vicuna-7B-v1.5-GGUF\"\n",
        "        filename=\"vicuna-7b-v1.5.Q4_K_M.gguf\"\n",
        "  elif(ver==\"Falcon-7B-Instruct\"):\n",
        "        repo_id=\"TheBloke/Falcon-7B-Instruct-GGML\"\n",
        "        filename=\"falcon-7b-instruct.ggccv1.q4_1.bin\"\n",
        "  elif(ver==\"CodeLlama-7B\"):\n",
        "        repo_id=\"TheBloke/CodeLlama-7B-GGUF\"\n",
        "        filename=\"codellama-7b.Q4_K_M.gguf\"\n",
        "  if Use_Temp_Storage:\n",
        "      os.makedirs('/content/models', exist_ok=True)\n",
        "      model='/content/models/'+ver\n",
        "  else:\n",
        "      model=MODELS_PATH+ver\n",
        "  link=repo_id\n",
        "  if not os.path.exists(model):\n",
        "    model_path = hf_hub_download(\n",
        "    repo_id= repo_id,\n",
        "    filename= filename,\n",
        "    resume_download=True,\n",
        "    cache_dir=MODELS_PATH,)\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "      clear_output()\n",
        "      inf('\\u2714 Done','success', '50px')\n",
        "    else:\n",
        "      inf('\\u2718 Something went wrong, try again','danger', \"250px\")\n",
        "  else:\n",
        "      clear_output()\n",
        "      inf('\\u2714 Model already exists','primary', '300px')\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "inf('\\u2718 Downloading files','success', \"400px\")\n",
        "PATH_to_MODEL=llmdl(Model_Version, Use_Temp_Storage)\n",
        "\n",
        "if os.path.exists(str(PATH_to_MODEL)):\n",
        "  inf('\\u2714 Using the custom model.','success', '200px')\n",
        "  model=PATH_to_MODEL\n",
        "\n",
        "#@markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CEjnDrp4qMrd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Start Chatbot\n",
        "from IPython.utils import capture\n",
        "import time\n",
        "import sys\n",
        "import fileinput\n",
        "from pyngrok import ngrok, conf\n",
        "import re\n",
        "\n",
        "Use_Cloudflare_Tunnel = False #@param {type:\"boolean\"}\n",
        "#@markdown - Offers better gradio responsivity\n",
        "\n",
        "Ngrok_token = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - Input your ngrok token if you want to use ngrok server\n",
        "\n",
        "User = \"\" #@param {type:\"string\"}\n",
        "Password= \"\" #@param {type:\"string\"}\n",
        "#@markdown - Add credentials to your Gradio interface (optional)\n",
        "\n",
        "auth=f\"--gradio-auth {User}:{Password}\"\n",
        "if User ==\"\" or Password==\"\":\n",
        "  auth=\"\"\n",
        "\n",
        "\n",
        "#with capture.capture_output() as cap:\n",
        "#  %cd /content/gdrive/$mainpth/llm/chatbot/modules/\n",
        "\n",
        "share=''\n",
        "if Ngrok_token!=\"\":\n",
        "  ngrok.kill()\n",
        "  srv=ngrok.connect(7860, pyngrok_config=conf.PyngrokConfig(auth_token=Ngrok_token) , bind_tls=True).public_url\n",
        "\n",
        "  for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "    if line.strip().startswith('self.server_name ='):\n",
        "        line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "    if line.strip().startswith('self.protocol = \"https\"'):\n",
        "        line = '            self.protocol = \"https\"\\n'\n",
        "    if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "        line = ''\n",
        "    if line.strip().startswith('else \"http\"'):\n",
        "        line = ''\n",
        "    sys.stdout.write(line)\n",
        "\n",
        "elif Use_Cloudflare_Tunnel:\n",
        "  with capture.capture_output() as cap:\n",
        "    !pkill cloudflared\n",
        "    time.sleep(4)\n",
        "    !nohup cloudflared tunnel --url http://localhost:7860 > /content/srv.txt 2>&1 &\n",
        "    time.sleep(4)\n",
        "    with open('/content/srv.txt', \"r\") as file: text = file.read()\n",
        "    srv= re.findall(r\"https?://(?:\\S+?\\.)?trycloudflare\\.com\\S*\", text)[0]\n",
        "\n",
        "    for line in fileinput.input('/usr/local/lib/python3.10/dist-packages/gradio/blocks.py', inplace=True):\n",
        "      if line.strip().startswith('self.server_name ='):\n",
        "          line = f'            self.server_name = \"{srv[8:]}\"\\n'\n",
        "      if line.strip().startswith('self.protocol = \"https\"'):\n",
        "          line = '            self.protocol = \"https\"\\n'\n",
        "      if line.strip().startswith('if self.local_url.startswith(\"https\") or self.is_colab'):\n",
        "          line = ''\n",
        "      if line.strip().startswith('else \"http\"'):\n",
        "          line = ''\n",
        "      sys.stdout.write(line)\n",
        "\n",
        "    !rm /content/srv.txt\n",
        "\n",
        "else:\n",
        "  share='--share'\n",
        "\n",
        "try:\n",
        "  if os.path.isfile(model):\n",
        "    !python /content/gdrive/$mainpth/llm/chatbot/app.py\n",
        "  else:\n",
        "    !python /content/gdrive/$mainpth/llm/chatbot/app.py\n",
        "except:\n",
        "   !python /content/gdrive/$mainpth/llm/chatbot/app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "s9gtqPNm2UpV",
        "outputId": "62db8b53-96d2-493c-c284-1dbbe152f823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from ./models/models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF/snapshots/3a6fbf4a41a1d52e415a4958cde6856d34b2db93/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
            "...............................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 3900\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
            "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
            "llama_new_context_with_model:        CPU input buffer size   =    15.64 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   294.53 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 1\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
            "Using chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "Using chat eos_token: \n",
            "Using chat bos_token: \n",
            "/usr/local/lib/python3.10/dist-packages/gradio/components/dropdown.py:163: UserWarning: The value passed into gr.Dropdown() is not in the list of choices. Please update the list of choices to include: ['CodeLlama-7B', 'Llama-2-13B-chat', 'Falcon-7B-InstructMistral-7B-Instruct-v0.2', 'zephyr-7B-beta', 'vicuna-7B-v1.5'] or set allow_custom_value=True.\n",
            "  warnings.warn(\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://b27d82c28da195b239.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "\n",
            "llama_print_timings:        load time =   47214.20 ms\n",
            "llama_print_timings:      sample time =      56.13 ms /    78 runs   (    0.72 ms per token,  1389.58 tokens per second)\n",
            "llama_print_timings: prompt eval time =   47213.87 ms /    67 tokens (  704.68 ms per token,     1.42 tokens per second)\n",
            "llama_print_timings:        eval time =   64478.72 ms /    77 runs   (  837.39 ms per token,     1.19 tokens per second)\n",
            "llama_print_timings:       total time =  112222.49 ms /   144 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "piCWkkg3s58S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p6ZvbA64wI1_"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}